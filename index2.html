<!DOCTYPE html>
<html>

<head>
    <meta name="viewport"
        content="width=device-width, user-scalable=no, initial-scale=1.0, maximum-scale=10.0, minimum-scale=0.1">

</head>

<body>
    <div id="main">
        <div style="position: relative">
            <video id="webcamStream" autoplay playsinline></video>
            <canvas id="videoCanvas"></canvas>
        </div>
        <div style="position: relative">
            <button id="startButton" style="height:55px; width:70px;">Start</button>
        </div>
    </div>

    <style>
        #main {
            display: grid;
            grid-template-columns: repeat(2, 1fr);
            column-gap: 1cm;
        }

        #webcamStream,
        #videoCanvas {
            position: static;
        }

        #webcamStream {
            z-index: 1;
            width: 50vw;
            height: auto;
            transform: rotateY(180deg);
            -webkit-transform: rotateY(180deg);
            -moz-transform: rotateY(180deg);

            /* maintain the aspect ratio */
            /*
            transform: scale(0.5);
            height:300px; 
            width:200px;
            */
        }

        #videoCanvas {
            position: absolute;
            top: 0;
            left: 0;
            z-index: 2;
            transform: rotateY(180deg);
            -webkit-transform: rotateY(180deg);
            -moz-transform: rotateY(180deg);

        }

        #startButton {
            z-index: 3;
        }
    </style>

    <script src="https://sdk.morphcast.com/mphtools/v1.1/mphtools.js"
        data-config="cameraPrivacyPopup, compatibilityUI, compatibilityAutoCheck"></script>
    <script src="https://ai-sdk.morphcast.com/v1.16/ai-sdk.js"></script>
    <script src="https://sdk.morphcast.com/emotion-statistics/v1.0-beta/script.js"></script>



    <script>
        var canvas = document.getElementById('videoCanvas');
        var context = canvas.getContext('2d');
        // Function to draw rectangles on the canvas
        function drawRectangles(y, x, width, height) {
            context.clearRect(0, 0, canvas.width, canvas.height);
            //faceData.forEach(function (face) {
            context.beginPath();
            context.rect(x, y, width, height);
            context.strokeStyle = 'green';
            context.stroke();
            //});
        }

        async function startWebcam() {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ video: true });
                const videoElement = document.getElementById('webcamStream');
                videoElement.srcObject = stream;


            } catch (error) {
                console.error('Error accessing webcam:', error);
            }
        }
        // Call the function to start the webcam stream
        startWebcam();

        //for face rectangles
        window.addEventListener('resize', resizeCanvas);
        function resizeCanvas() {
            var video = document.getElementById('webcamStream');
            var canvas = document.getElementById('videoCanvas');

            // Set the canvas dimensions to match the video element
            console.log("video height: ", video.clientHeight);
            canvas.width = video.clientWidth;
            canvas.height = video.clientHeight;
        }
        // Call the function to initially set the size
        resizeCanvas();
    </script>

    <!-- Add a button for the user to start the video -->
    <h1>Emotion ai</h1>
    <div id="apiOutput">Press start button to get the data</div>

    <script>
        // Function to call the Orphcast API and display the result
        function fetchMorphcastAPI() {
            // Define the API endpoint
            const apiEndpoint = 'YOUR_API_ENDPOINT';

            // Make the API call 
            fetch(apiEndpoint, {
                method: 'GET', // or 'POST' if required
                headers: {
                    // Add any required headers here
                }
            })
                .then(response => response.json())
                .then(data => {
                    // Assuming the API returns a JSON object with the parameters
                    const output = `
          <p>Emotion: ${data.emotion.face_detector.totalFaces.toString()}</p>
          <p>Attention: ${data.attention.face_emotion.dominantEmotion}</p>
          <p>Parameter 3: ${data.param3}</p>
          <p>Parameter 4: ${data.param4}</p> 
          <p>Parameter 5: ${data.param5}</p>
        `;
                    document.getElementById('apiOutput').innerHTML = output;
                    //console.log("--------------", data.emotion.face_detector.totalFaces);
                    //console.log("--------------", data.attention.face_emotion.dominantEmotion);
                })
                .catch(error => {
                    console.error('Error fetching the Orphcast API:', error);
                    document.getElementById('apiOutput').textContent = 'Error loading output.';
                });
        }

        // Call the function on page load
        //window.onload = fetchMorphcastAPI;
    </script>
    <script>
        // ... your existing script ...

        // Add an event listener to the start button
        document.getElementById('startButton').addEventListener('click', function () {
            var video = document.getElementById('webcamStream');

            console.log("video height on start click: ", video.clientHeight);
            resizeCanvas();

            const statsConfig = {
                sendDatainterval: 5000,
                tickInterval: 1000,
                stopAfter: 7200000,
                licenseKey: "sk300739885aa80c0ecc47487a732aa36e3e4664293eba"
            };
            const statisticsUploader = new MorphCastStatistics.StatisticsUploader(statsConfig);

            CY.loader()
                .licenseKey("sk300739885aa80c0ecc47487a732aa36e3e4664293eba")
                //.source(CY.createSource.fromVideoUrl("https://github.com/FaigBagirov/emotionai_test/raw/main/mixkit-businessman-talking-to-an-audience-23318-medium.mp4"))
                .addModule(CY.modules().FACE_AROUSAL_VALENCE.name, { smoothness: 0.70 })
                .addModule(CY.modules().FACE_EMOTION.name, { smoothness: 0.40 })
                .addModule(CY.modules().FACE_ATTENTION.name, { smoothness: 0.83 })
                .addModule(CY.modules().ALARM_LOW_ATTENTION.name, { timeWindowMs: 5000, initialToleranceMs: 7000, threshold: 0.33 })
                .addModule(CY.modules().FACE_WISH.name, { smoothness: 0.8 })
                .addModule(CY.modules().FACE_POSE.name, { smoothness: 0.65 })
                .addModule(CY.modules().FACE_DETECTOR.name, { maxInputFrameSize: 320, smoothness: 0.83 })
                .addModule(CY.modules().DATA_AGGREGATOR.name, { initialWaitMs: 2000, periodMs: 1000 })
                .addModule(CY.modules().FACE_POSITIVITY.name, { smoothness: 0.40, gain: 2, angle: 17 })
                .load()
                .then(async ({ start, stop }) => {

                    await start();
                    await statisticsUploader.start();

                    setTimeout(async () => {
                        await statisticsUploader.stop();
                        await stop();
                    }, statsConfig.stopAfter); //stops after 120 mins 

                });

            window.addEventListener(CY.modules().EVENT_BARRIER.eventName, (evt) => {
                //console.log('EVENT_BARRIER result', evt.detail);
                let dominantEmotion = "--";
                let positivity = "--";
                let attention = "--";
                let faceCoordinates = {
                    y: "--",
                    x: "--",
                    width: "--",
                    height: "--",
                    confidence: "--",
                };

                if (evt.detail.face_emotion) {
                    dominantEmotion = evt.detail.face_emotion.dominantEmotion;
                }
                if (evt.detail.face_positivity) {
                    positivity = evt.detail.face_positivity.positivity;
                }
                if (evt.detail.face_attention) {
                    attention = evt.detail.face_attention.attention;
                }

                evt.detail.face_detector.rects.forEach(rect => {
                    // Calculate the top left corner of the rectangle

                    faceCoordinates.width = rect.width;
                    faceCoordinates.height = rect.height;
                    faceCoordinates.y = rect.y;
                    faceCoordinates.x = rect.x;

                    var centerY = faceCoordinates.y + faceCoordinates.height / 2;
                    var centerX = faceCoordinates.x + faceCoordinates.width / 2;

                    var rectY = centerY - faceCoordinates.height / 2;
                    var rectX = centerX - faceCoordinates.width / 2;

                    faceCoordinates.confidence = rect.confidence.toString().slice(0, 10);

                    drawRectangles(rectY, rectX, faceCoordinates.width, faceCoordinates.height);
                });


                //all features chosen by me in the configurator ui
                // FACE_AROUSAL_VALENCE
                // FACE_EMOTION
                // FACE_ATTENTION
                // ALARM_LOW_ATTENTION
                // FACE_WISH
                // FACE_POSE
                // FACE_DETECTOR
                // DATA_AGGREGATOR
                // FACE_POSITIVITY
                const output = `
                <p>Faces count: ${evt.detail.face_detector.totalFaces}</p>
                <p>Emotion: ${dominantEmotion}</p>
                <p>Face positivity: ${positivity}</p>
                <p>Face attention: ${attention}</p>

                <p>Face coordinates y: ${faceCoordinates.y}</p>
                <p>Face coordinates x: ${faceCoordinates.x}</p>
                <p>Face coordinates w: ${faceCoordinates.width}</p>
                <p>Face coordinates h: ${faceCoordinates.height}</p>
                <p>Face coordinates confidence: ${faceCoordinates.confidence}</p>


        `;
                document.getElementById('apiOutput').innerHTML = output;

            });

            window.addEventListener(CY.modules().DATA_AGGREGATOR.eventName, (evt) => {
                //console.log('DATA_AGGREGATOR result', evt.detail);
            });

            MphTools.CameraPrivacyPopup.setText({
                "title": "Allow us to use your camera",
                "description": "This experience is designed to be viewed with your camera on. The next screen will ask your consent to access data from your camera.",
                "url": "https://YOUR_DOMAIN/<YOUR_PRIVACY_POLICY>"
            });

            //   CY.loader()
            //     .source(CY.createSource.fromVideoUrl("https://cors-anywhere.herokuapp.com/https://github.com/FaigBagirov/emotionai_test/raw/main/mixkit-businessman-talking-to-an-audience-23318-medium.mp4"))
            //     // ... rest of your code ...
            //     .then(async ({ start, stop }) => {

            //       await start();
            //       await statisticsUploader.start();

            //       // Hide the play button after starting the video
            //       document.getElementById('startButton').style.display = 'none';

            //       setTimeout(async () => {
            //           await statisticsUploader.stop();
            //           await stop();
            //       }, statsConfig.stopAfter); //stops after 120 mins 

            //     });
        });
    </script>

    <!-- <p id="first" style="display:block">Lorem ipsum dolor sit amet, consectetur adipiscing elit.</p>
<p id="second">Vivamus semper eleifend lectus, sed faucibus massa cursus a.</p>

Replace HTML using Javascript -->
    <script>
        //   window.addEventListener("load", () => {
        //     // Replace the entire <p> element with <strong>
        //     var first = document.getElementById("first");
        //     first.outerHTML = "<strong>FOO BAR!</strong>";

        //     // Change the content of the second <p> element
        //     var second = document.getElementById("second");
        //     second.innerHTML = "<u>FOO</u> <i>BAR</i>";
        //   });
    </script>





</body>

</html>