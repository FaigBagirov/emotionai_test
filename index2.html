<!DOCTYPE html>
<html>

<head>

</head>

<body>

    <video id="webcamStream" autoplay playsinline style="height:300px; width:200px;display: flex;"></video>
    <canvas id="videoCanvas" width="300" height="200"></canvas>
    <script>
        // Function to draw rectangles on the canvas
        function drawRectangles(y,x,width,height) {
            context.clearRect(0, 0, canvas.width, canvas.height);
            faceData.forEach(function (face) {
                context.beginPath();
                context.rect(x, y, width, height);
                context.strokeStyle = 'green';
                context.stroke();
            });
        }

        async function startWebcam() {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ video: true });
                const videoElement = document.getElementById('webcamStream');
                videoElement.srcObject = stream;

                //for face rectangles
                var canvas = document.getElementById('videoCanvas');
                var context = canvas.getContext('2d');
            } catch (error) {
                console.error('Error accessing webcam:', error);
            }
        }

        // Call the function to start the webcam stream
        startWebcam();
    </script>
    <script src="https://sdk.morphcast.com/mphtools/v1.1/mphtools.js"
        data-config="cameraPrivacyPopup, compatibilityUI, compatibilityAutoCheck"></script>
    <script src="https://ai-sdk.morphcast.com/v1.16/ai-sdk.js"></script>
    <script src="https://sdk.morphcast.com/emotion-statistics/v1.0-beta/script.js"></script>
    <!-- ... your existing code ... -->

    <!-- Add a button for the user to start the video -->
    <button id="playButton" style="height:55px; width:70px;">Start</button>
    <h1>Emotion ai</h1>
    <div id="apiOutput">Press start button to get the data</div>

    <script>
        // Function to call the Orphcast API and display the result
        function fetchMorphcastAPI() {
            // Define the API endpoint
            const apiEndpoint = 'YOUR_API_ENDPOINT';

            // Make the API call 
            fetch(apiEndpoint, {
                method: 'GET', // or 'POST' if required
                headers: {
                    // Add any required headers here
                }
            })
                .then(response => response.json())
                .then(data => {
                    // Assuming the API returns a JSON object with the parameters
                    const output = `
          <p>Emotion: ${data.emotion.face_detector.totalFaces.toString()}</p>
          <p>Attention: ${data.attention.face_emotion.dominantEmotion}</p>
          <p>Parameter 3: ${data.param3}</p>
          <p>Parameter 4: ${data.param4}</p> 
          <p>Parameter 5: ${data.param5}</p>
        `;
                    document.getElementById('apiOutput').innerHTML = output;
                    //console.log("--------------", data.emotion.face_detector.totalFaces);
                    //console.log("--------------", data.attention.face_emotion.dominantEmotion);
                })
                .catch(error => {
                    console.error('Error fetching the Orphcast API:', error);
                    document.getElementById('apiOutput').textContent = 'Error loading output.';
                });
        }

        // Call the function on page load
        //window.onload = fetchMorphcastAPI;
    </script>
    <script>
        // ... your existing script ...

        // Add an event listener to the play button
        document.getElementById('playButton').addEventListener('click', function () {

            const statsConfig = {
                sendDatainterval: 5000,
                tickInterval: 1000,
                stopAfter: 7200000,
                licenseKey: "sk300739885aa80c0ecc47487a732aa36e3e4664293eba"
            };
            const statisticsUploader = new MorphCastStatistics.StatisticsUploader(statsConfig);

            CY.loader()
                .licenseKey("sk300739885aa80c0ecc47487a732aa36e3e4664293eba")
                //.source(CY.createSource.fromVideoUrl("https://github.com/FaigBagirov/emotionai_test/raw/main/mixkit-businessman-talking-to-an-audience-23318-medium.mp4"))
                .addModule(CY.modules().FACE_AROUSAL_VALENCE.name, { smoothness: 0.70 })
                .addModule(CY.modules().FACE_EMOTION.name, { smoothness: 0.40 })
                .addModule(CY.modules().FACE_ATTENTION.name, { smoothness: 0.83 })
                .addModule(CY.modules().ALARM_LOW_ATTENTION.name, { timeWindowMs: 5000, initialToleranceMs: 7000, threshold: 0.33 })
                .addModule(CY.modules().FACE_WISH.name, { smoothness: 0.8 })
                .addModule(CY.modules().FACE_POSE.name, { smoothness: 0.65 })
                .addModule(CY.modules().FACE_DETECTOR.name, { maxInputFrameSize: 320, smoothness: 0.83 })
                .addModule(CY.modules().DATA_AGGREGATOR.name, { initialWaitMs: 2000, periodMs: 1000 })
                .addModule(CY.modules().FACE_POSITIVITY.name, { smoothness: 0.40, gain: 2, angle: 17 })
                .load()
                .then(async ({ start, stop }) => {

                    await start();
                    await statisticsUploader.start();

                    setTimeout(async () => {
                        await statisticsUploader.stop();
                        await stop();
                    }, statsConfig.stopAfter); //stops after 120 mins 

                });

            window.addEventListener(CY.modules().EVENT_BARRIER.eventName, (evt) => {
                //console.log('EVENT_BARRIER result', evt.detail);
                let dominantEmotion = "--";
                let positivity = "--";
                let attention = "--";
                let faceCoordinates = {
                    y: "--",
                    x: "--",
                    width: "--",
                    height: "--",
                    confidence: "--",
                };

                if (evt.detail.face_emotion) {
                    dominantEmotion = evt.detail.face_emotion.dominantEmotion;
                }
                if (evt.detail.face_positivity) {
                    positivity = evt.detail.face_positivity.positivity;
                }
                if (evt.detail.face_attention) {
                    attention = evt.detail.face_attention.attention;
                }
                if (evt.detail.face_detector.rects[0]) {
                    faceCoordinates.y = evt.detail.face_detector.rects[0].y.toString().slice(0, 6);
                    faceCoordinates.x = evt.detail.face_detector.rects[0].x.toString().slice(0, 6);
                    faceCoordinates.width = evt.detail.face_detector.rects[0].width.toString().slice(0, 6);
                    faceCoordinates.height = evt.detail.face_detector.rects[0].height.toString().slice(0, 6);
                    faceCoordinates.confidence = evt.detail.face_detector.rects[0].confidence.toString().slice(0, 6);

                    drawRectangles(y,x,width,height);
                }

                //all features chosen by me in the configurator ui
                // FACE_AROUSAL_VALENCE
                // FACE_EMOTION
                // FACE_ATTENTION
                // ALARM_LOW_ATTENTION
                // FACE_WISH
                // FACE_POSE
                // FACE_DETECTOR
                // DATA_AGGREGATOR
                // FACE_POSITIVITY
                const output = `
                <p>Faces count: ${evt.detail.face_detector.totalFaces}</p>
                <p>Emotion: ${dominantEmotion}</p>
                <p>Face positivity: ${positivity}</p>
                <p>Face attention: ${attention}</p>

                <p>Face coordinates y: ${faceCoordinates.y}</p>
                <p>Face coordinates x: ${faceCoordinates.x}</p>
                <p>Face coordinates w: ${faceCoordinates.width}</p>
                <p>Face coordinates h: ${faceCoordinates.height}</p>
                <p>Face coordinates confidence: ${faceCoordinates.confidence}</p>


        `;
                document.getElementById('apiOutput').innerHTML = output;

            });

            window.addEventListener(CY.modules().DATA_AGGREGATOR.eventName, (evt) => {
                //console.log('DATA_AGGREGATOR result', evt.detail);
            });

            MphTools.CameraPrivacyPopup.setText({
                "title": "Allow us to use your camera",
                "description": "This experience is designed to be viewed with your camera on. The next screen will ask your consent to access data from your camera.",
                "url": "https://YOUR_DOMAIN/<YOUR_PRIVACY_POLICY>"
            });

            //   CY.loader()
            //     .source(CY.createSource.fromVideoUrl("https://cors-anywhere.herokuapp.com/https://github.com/FaigBagirov/emotionai_test/raw/main/mixkit-businessman-talking-to-an-audience-23318-medium.mp4"))
            //     // ... rest of your code ...
            //     .then(async ({ start, stop }) => {

            //       await start();
            //       await statisticsUploader.start();

            //       // Hide the play button after starting the video
            //       document.getElementById('playButton').style.display = 'none';

            //       setTimeout(async () => {
            //           await statisticsUploader.stop();
            //           await stop();
            //       }, statsConfig.stopAfter); //stops after 120 mins 

            //     });
        });
    </script>

    <!-- <p id="first" style="display:block">Lorem ipsum dolor sit amet, consectetur adipiscing elit.</p>
<p id="second">Vivamus semper eleifend lectus, sed faucibus massa cursus a.</p>

Replace HTML using Javascript -->
    <script>
        //   window.addEventListener("load", () => {
        //     // Replace the entire <p> element with <strong>
        //     var first = document.getElementById("first");
        //     first.outerHTML = "<strong>FOO BAR!</strong>";

        //     // Change the content of the second <p> element
        //     var second = document.getElementById("second");
        //     second.innerHTML = "<u>FOO</u> <i>BAR</i>";
        //   });
    </script>





</body>

</html>